{"metadata":{"colab":{"provenance":[],"collapsed_sections":["6CZi9xGqLgpo","5ujD_S7_LugH","Q3ZZpJ40MT0t","dSWcSXyfX0Rv","Ok47_snkL2aH","UbYr3wJ-XUo6","gNgjcqRSYxtd"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Explore CLEAR raw data","metadata":{"id":"hHnlXD4AMBDu"}},{"cell_type":"markdown","source":"## 1.0 Utils","metadata":{"id":"UyzHGO0u_1k2"}},{"cell_type":"code","source":"def df_to_json(df, path, name):\n  timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n  file_name = f\"{path}/{name}_{timestamp}.json\"\n  df.to_json(file_name, orient='records')\n  print(f\"{name} saved to file: {file_name}\")","metadata":{"id":"XpEKJbxL_791"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_json_with_frozenset_column(path, frozenset_column_name):\n  df = pd.read_json(path, orient='records')\n  df[frozenset_column_name] = df[frozenset_column_name].apply(lambda x: frozenset(x))\n  return df","metadata":{"id":"zVTLK4H8IhiH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 Requirements","metadata":{"id":"PNRCFGPU34jZ"}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport json\nimport ast\nimport random\nimport csv\n\nfrom random import sample\nfrom datetime import datetime","metadata":{"id":"iOOszZAsEeyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"SU4s8w7cIHvk","outputId":"e73b1b50-6091-41fa-c7e7-63ca7182241e","execution":{"iopub.status.busy":"2023-11-14T16:41:58.090328Z","iopub.execute_input":"2023-11-14T16:41:58.091210Z","iopub.status.idle":"2023-11-14T16:41:58.415395Z","shell.execute_reply.started":"2023-11-14T16:41:58.091172Z","shell.execute_reply":"2023-11-14T16:41:58.414185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Moshiii/CLEAR-replication.git","metadata":{"id":"245qNHcaIo32","outputId":"e956838d-b425-409b-da34-a5e626922e94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Explore BIKER_train.QApair.csv","metadata":{"id":"6CZi9xGqLgpo"}},{"cell_type":"code","source":"qa_dataset = pd.read_csv(\"CLEAR-replication/data/BIKER_train.QApair.csv\")\nqa_dataset = qa_dataset[['title', 'answer']]\nqa_dataset.head()","metadata":{"id":"VxK6cXXHFEV4","outputId":"599926ee-5578-4530-8a86-78adc80bad97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_dataset.shape","metadata":{"id":"tl56IssxJmwv","outputId":"2af8c290-293b-4e5e-d7a0-f6c89303f291"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(qa_dataset))","metadata":{"id":"3m7_s4YEZOXm","outputId":"7e7244dd-8078-431f-d10c-3c9f70eaad3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = list(qa_dataset['title'].unique())\nlen(question)","metadata":{"id":"9VC6EL0kXtpi","outputId":"4e53175f-0702-446f-f3a2-e8fdd33082dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer = list(qa_dataset['answer'].unique())\nlen(answer)","metadata":{"id":"wG6Wi6LzaBYR","outputId":"dff0b5c9-05e9-4691-fee3-3c8ddcfaf84c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove duplicate sample\nqa_dataset = qa_dataset.drop_duplicates(subset=['title'], keep='first')\nqa_dataset.shape","metadata":{"id":"tRpocrt6CH0U","outputId":"f859f46a-0722-4215-d002-f39a6d865afb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Explore Biker_test_filtered.csv","metadata":{"id":"5ujD_S7_LugH"}},{"cell_type":"code","source":"biker_test = pd.read_csv(\"CLEAR-replication/data/Biker_test_filtered.csv\")\nbiker_test = biker_test[['title', 'answer']]\nbiker_test.head()","metadata":{"id":"uV9-B4BLLABq","outputId":"3f4f670a-8b16-4d89-f234-71ec6ff2a59a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"biker_test.shape","metadata":{"id":"pXrUuWmxLXUt","outputId":"d8c3ec16-2bdb-4da4-d901-69cccc3d7fb3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicate samples\nbiker_test = biker_test.drop_duplicates(subset=['title'], keep='first')\nbiker_test.shape","metadata":{"id":"DtJLPtzuL-tz","outputId":"48f3c50e-8eb6-47cc-d6c4-18fb98e05baa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"queries = biker_test[\"title\"].to_list()\nqueries_answers = biker_test[\"answer\"].to_list()\nqueries_answers=[str(list(eval(x))) for x in queries_answers]","metadata":{"id":"GXmPZZOqQmKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(queries)","metadata":{"id":"jP4xU5EFQuKS","outputId":"752aa611-77e9-456a-fdc5-f48c57da78b9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3.1 Check duplicate between qa_dataset and biker_test","metadata":{"id":"RBebBZnPxx9V"}},{"cell_type":"code","source":"dup = qa_dataset.merge(biker_test, on=['title', 'answer'])","metadata":{"id":"5WfxszA3vra5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not dup.empty:\n  print(len(dup)  )\nelse:\n  print(\"no dup\")","metadata":{"id":"M3TpB3nwv_7n","outputId":"01e161fe-c2d6-44cf-a1c7-8ad9c0abeb05"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3.2 Check duplicate between Answer_dict and biker_test answer","metadata":{"id":"ArXR6D7Rx-Ah"}},{"cell_type":"code","source":"data_folder = 'CLEAR-replication/data/full_data_min_5_max_10_ir_10'","metadata":{"id":"6UsIDWFlx9ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Answers_dict = {}\ncollection_filepath = os.path.join(data_folder, 'Answers_dict.json')\nwith open(collection_filepath, 'r', encoding='utf8') as fIn:\n  Answers_dict = json.load(fIn)","metadata":{"id":"IFZ7UAvVyQlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_answers = []\n\nAnswers_values = Answers_dict.values()\nfor v in Answers_values:\n  unique_answers.append(v)\n\nunique_answers = list(set(unique_answers))\nprint(unique_answers)","metadata":{"id":"ERh5xOKNzKJl","outputId":"9124c6ce-f3b4-4ab6-e4c6-c669d852e67c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(queries_answers)","metadata":{"id":"JBcYT8AuzB2g","outputId":"d69eb0dc-bf77-4734-f1de-a9cc1aba39d7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list1 = queries_answers\nlist2 = unique_answers\n\n# Convert string representations to actual lists\nlist1 = [eval(item) for item in list1]\nlist2 = [eval(item) for item in list2]\n\n# Convert lists to sets for efficient duplicate checking\nset1 = set(tuple(item) for item in list1)\nset2 = set(tuple(item) for item in list2)\n\n# Find duplicates by taking the intersection of the sets\nduplicates = set1.intersection(set2)\n\n# Convert the duplicates back to lists\nduplicates = [list(item) for item in duplicates]\n\n# Print the duplicates\nprint(\"Duplicates numbers:\", len(duplicates))","metadata":{"id":"93_SZfxRzPUH","outputId":"2b5d1b56-535f-49bb-a460-320a80f0a1b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 ----------Combine QA dataset and biker test","metadata":{"id":"Q3ZZpJ40MT0t"}},{"cell_type":"code","source":"pair_dataset = pd.concat([qa_dataset, biker_test], axis=0, ignore_index=True)\npair_dataset.shape","metadata":{"id":"zZ5ltXE7Mcib","outputId":"44c0d11f-c00c-49f0-8a7e-6ffc0afd3624"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicate samples\npair_dataset_filtered = pair_dataset.drop_duplicates(subset=['title'], keep='first')\npair_dataset_filtered.shape","metadata":{"id":"dLKFc08pNF4W","outputId":"391b9b22-4431-4eaf-d966-29bb4749dba4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_answers = list(pair_dataset_filtered['answer'].unique())\nprint(len(all_answers))","metadata":{"id":"skrN1i4-fRJe","outputId":"ec038272-3850-4d57-f087-2d31e79b56f7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> biker_test file answers are all in the training data","metadata":{"id":"TdiKeYz9gMO3"}},{"cell_type":"markdown","source":"## 1.5 ------------Filter out multi API sample","metadata":{"id":"dSWcSXyfX0Rv"}},{"cell_type":"code","source":"mul_dataset = pair_dataset_filtered[pair_dataset_filtered['answer'].str.contains(\",\")]\nmul_dataset","metadata":{"id":"8XqpANrEX9Mf","outputId":"bf1c5c50-eaef-4122-a81d-362bea58b254"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mul_dataset.shape)","metadata":{"id":"Ecvn7N_6Y2FK","outputId":"d6f27cea-686c-4c5b-eaaf-e41f2c495ce3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.6 Explore SO_test_filtered.csv","metadata":{"id":"Ok47_snkL2aH"}},{"cell_type":"code","source":"SO_test = pd.read_csv(\"CLEAR-replication/data/SO_test_filtered.csv\")\nSO_test = SO_test[['title', 'answer']]\nSO_test.head()","metadata":{"id":"Tyeomw1DL9of","outputId":"6e24fbd3-d438-47db-9282-501ba1d1811b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SO_test.shape","metadata":{"id":"UGfgYq8hMknY","outputId":"12160148-ab67-4c67-9763-d3cde891620f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SO_test_filtered = SO_test.drop_duplicates(subset=['title'], keep='first')\nSO_test_filtered.shape","metadata":{"id":"DZB6JbEWb2E3","outputId":"22e768bc-ce63-4a75-e0b8-d7829120a0e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pair_dataset1 = pd.concat([pair_dataset_filtered, SO_test_filtered], axis=0, ignore_index=True)\npair_dataset1.shape","metadata":{"id":"Hu1Qg33icJHB","outputId":"6b8bfa95-7d34-4dee-e647-52230b427dbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pair_dataset1_filtered = pair_dataset1.drop_duplicates(subset=['title'], keep='first')\npair_dataset1_filtered.shape","metadata":{"id":"-wfUkSSvdE66","outputId":"ab94cee3-7b92-4434-a39f-023a79be772a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> SO_test doesn't have any duplicate samples with (bikertrain+bikertest)\nnote: bikertrain has some duplicate sample with bikertest","metadata":{"id":"x7kgh7-bda0Z"}},{"cell_type":"code","source":"pair1_filtered_answers = list(pair_dataset1_filtered['answer'].unique())\nprint(len(pair1_filtered_answers))","metadata":{"id":"KZn_wr4ugagt","outputId":"6e1ecb46-a840-4a6e-c686-b8e933b1fd0e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> SO_test's answers are all already in training data","metadata":{"id":"qThuM3Amgt3U"}},{"cell_type":"markdown","source":"## 1.7 Explore test_queries_min_5_max_10_ir_10.csv","metadata":{"id":"UbYr3wJ-XUo6"}},{"cell_type":"code","source":"test_queries = pd.read_csv('CLEAR-replication/data/test_queries_min_5_max_10_ir_10.csv')\ntest_queries = test_queries[['title', 'answer']]\ntest_queries.head()","metadata":{"id":"HwRryXUOXhV4","outputId":"988d839e-b4af-4fb6-e040-555c75aad09b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_queries.shape","metadata":{"id":"hl-TGr1SYg5N","outputId":"c2d762b4-1f2f-4904-dff7-f77edae8a5a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_queries_filtered = test_queries.drop_duplicates(subset=['title'], keep='first')\ntest_queries_filtered.shape","metadata":{"id":"f4siXFEMjv5T","outputId":"e287a4bd-45e8-46e8-abe9-327c3b79207c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pair_dataset2 = pd.concat([pair_dataset_filtered, test_queries_filtered], axis=0, ignore_index=True)\npair_dataset2.shape","metadata":{"id":"RqCRuqmMkLD3","outputId":"64832488-e849-455a-f1d0-22dcb489fe3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pair_dataset2_filtered = pair_dataset2.drop_duplicates(subset=['title'], keep='first')\npair_dataset2_filtered.shape","metadata":{"id":"DE6HIsNqkpp1","outputId":"585fe70e-c22f-4735-b763-7c61e30a12df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> test_queries is a subset of training data","metadata":{"id":"ZpHm5a44k8ZX"}},{"cell_type":"code","source":"pair2_filtered_answers = list(pair_dataset2['answer'].unique())\nprint(len(pair2_filtered_answers))","metadata":{"id":"qBkz4BCyg6T_","outputId":"caead61c-34e5-4135-f2de-1f9e1a5d7def"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> test_queries's answers are all already in training data","metadata":{"id":"cW1EF-8xhI6Y"}},{"cell_type":"markdown","source":"## 1.8 Explore test_queries_multi_min_5_max_10_ir_10.csv","metadata":{"id":"gNgjcqRSYxtd"}},{"cell_type":"code","source":"test_mul_queries = pd.read_csv('CLEAR-replication/data/test_queries_multi_min_5_max_10_ir_10.csv')\ntest_mul_queries = test_mul_queries[['title', 'answer']]\ntest_mul_queries.head()","metadata":{"id":"G8tJ0GMdaaPM","outputId":"6dcf96ea-821a-42f1-eb0a-9d7f10df5ec8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> It's not multi queries for sure","metadata":{"id":"WnmbMPn0mEjg"}},{"cell_type":"code","source":"test_mul_queries.shape","metadata":{"id":"ECCVPR-da0lW","outputId":"e476aab0-20bd-4da1-f0c5-840985535e59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Explore CLEAR processed data","metadata":{"id":"HY2MlWjXxL_d"}},{"cell_type":"code","source":"processed_data_folder = 'CLEAR-replication/data/full_data_min_5_max_10_ir_10'","metadata":{"id":"E9nN3sP_xd6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.0 Replicate CLEAR data processing phase","metadata":{"id":"r503rl9N5g0c"}},{"cell_type":"code","source":"raw_data_folder = 'CLEAR-replication/data'\nevaluate_data_folder = 'drive/MyDrive/Lab RISE/CLEAR/data/evaluate'\noutput_data_folder = 'drive/MyDrive/Lab RISE/CLEAR/data/output'","metadata":{"id":"17ysn1dO5sYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# already run, this code is to create BIKER_querys_final\nimport csv\nimport re\nimport os\n\n# Input and output file paths\ninput_file = os.path.join(evaluate_data_folder, 'querys_text.txt')\noutput_file = os.path.join(os.path.dirname(input_file), 'BIKER_querys_final.csv')\n\n# Define a list to store the data\ndata = []\n\n# Read the input text file\nwith open(input_file, 'r') as f:\n    lines = f.readlines()\n\nquerys = list()\n\n# Process the lines and extract titles and answers\nfor line in lines:\n    if not line:\n        break\n    if len(line) <= 2 or '$$$$$' not in line:\n        continue\n\n    line = line.replace('\\n','')\n    line = line.split('$$$$$')\n\n    title = line[0].strip()\n    if title[0:2] == '**':\n        continue\n    if title[-1] == '?':\n        title = title[:-1]\n    apis = line[1].split(' ')\n    apis_list = list()\n    for api in apis:\n        if len(api.strip())>1:\n            apis_list.append(api)\n    querys.append((title,apis_list))\n\n# Write the data to a CSV file\nwith open(output_file, 'w', newline='') as csvfile:\n    csvwriter = csv.writer(csvfile)\n    csvwriter.writerow(['', 'title', 'answer'])\n    for i, (title, apis_list) in enumerate(querys):\n        csvwriter.writerow([i, title, str(apis_list)])\n\nprint(\"CSV file has been created: \", output_file)","metadata":{"id":"muIegcxx93ze","outputId":"22b1cd17-0520-4fcc-e27c-bdea39127611"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train1 = pd.read_csv(os.path.join(raw_data_folder, 'BIKER_train.QApair.csv'))\ndf_train1 = df_train1[['title', 'answer']]\ndf_train1.shape","metadata":{"id":"1cRK-PGsBTos","outputId":"56b5ce23-3ce4-443a-ab89-00000e11bb70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train1 = df_train1.drop_duplicates(subset=['title'], keep='first')\ndf_train1.shape","metadata":{"id":"poPka0ZkE-FN","outputId":"65b052c8-5601-4fbe-cf7f-e64eb4b35db5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train3 = pd.read_csv(os.path.join(evaluate_data_folder, 'BIKER_querys_final.csv'))\ndf_train3 = df_train3[['title', 'answer']]\ndf_train3.shape","metadata":{"id":"ETHM4O0CDLZ6","outputId":"28f7acfe-8571-4cf7-a366-ebf2242f1b79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train3 = df_train3.drop_duplicates(subset=['title'], keep='first')\ndf_train3.shape","metadata":{"id":"K3TzXlk2FK-1","outputId":"5c983477-852e-4db5-a79f-19416660aa95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3_multi=df_train3[df_train3['answer'].str.contains(\",\")]\nlen(df3_multi)","metadata":{"id":"Nm0pvSIWb2jO","outputId":"e289e4af-ca77-4e54-dae0-67c1e20bb372"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = [df_train1, df_train3]\ndf_train = pd.concat(df_train)\ndf_train.shape","metadata":{"id":"rX85f8lgDAEt","outputId":"04bdd43e-35cc-4683-e7a8-8c667d16d7d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop_duplicates(subset=['title'], keep='first')\ndf_train.shape","metadata":{"id":"Qd7ffYVxFe--","outputId":"dd4d6061-c08e-4506-90a2-b7adc0d17d1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_multi=df_train[df_train[\"answer\"].str.contains(\",\")]\nlen(df_multi)","metadata":{"id":"EB7puoBlF1tK","outputId":"87577f49-73dd-4da5-e3d2-60f3b9908077"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# moshii method to check all answer and it's wrong\nlen(list(set(df_train[\"answer\"].to_list())))","metadata":{"id":"LdO4X2WkbZEC","outputId":"24060d57-edea-410c-fd5a-60525599278c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_answer_list = df_train['answer'].to_list()\n# len(df_train_answer_list)\n\ndf_train_answer_set = set()\ndf_train_answer_set_list = []\n\nfor df_train_answer in df_train_answer_list:\n  df_train_answer_set.add(frozenset(ast.literal_eval(df_train_answer)))\n  df_train_answer_set_list.append(frozenset(ast.literal_eval(df_train_answer)))\n\nlen(df_train_answer_set)","metadata":{"id":"ZpMVwrkbRn6U","outputId":"d00dac3c-d2cf-460e-867f-d9f8768841d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['set_answer']=df_train_answer_set_list\ndf_train","metadata":{"id":"tmR4IFCsftxn","outputId":"23138705-e106-4440-cc92-77acaf4b7bf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_to_json(df_train, output_data_folder, 'df_train_with_set_answer')","metadata":{"id":"ayKreFdX8dIm","outputId":"688b5436-cbf4-473f-8fa6-c1a8ecae261a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\n# Create a set of frozensets\nset_of_sets = {frozenset([1, 2, 3]), frozenset([3, 4, 5]), frozenset([1, 2, 3])}\n\nset_of_sets.add(frozenset([3, 4, 5]))\n\n# Print the set of frozensets\nprint(set_of_sets)","metadata":{"id":"466We_TzTalg","outputId":"e378b526-199f-4071-f1ba-60161bfa4175"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nprint(df_train['answer'].dtypes)","metadata":{"id":"BC5xUepyOUmp","outputId":"ae7ef2ea-b956-4c8b-d994-9356baaeb6c4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nprint(['abc', 'def']==['def', 'abc'])","metadata":{"id":"fLDmPO3nO971","outputId":"fff1a960-0d60-40fd-d157-eac296f0ddb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nimport pandas as pd\nimport ast\n\n# Sample DataFrame with your data\ndata = {\n    'answer': [\"['abc', 'def']\", \"['def', 'abc']\", \"['abc', 'def', 'ghi']\"]\n}\ndf = pd.DataFrame(data)\n\n# Function to check if a list is equivalent to another list\ndef are_lists_equivalent(s1, s2):\n    try:\n        list1 = ast.literal_eval(s1)\n        list2 = ast.literal_eval(s2)\n        return set(list1) == set(list2)\n    except (SyntaxError, ValueError):\n        return False\n\n# Count the number of rows where \"answer\" lists are equivalent to any other list\ncount = 0\nfor i, row in df.iterrows():\n    for j, other_row in df.iterrows():\n        if i != j and are_lists_equivalent(row[\"answer\"], other_row[\"answer\"]):\n            count += 1\n\nprint(\"Number of rows with 'answer' lists that are equivalent to others:\", count)\n","metadata":{"id":"4E676K1oOROX","outputId":"35fd4661-cf6b-48dc-bce9-529022763557"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nimport pandas as pd\n\ndata = {'column_of_sets': [frozenset([1, 2, 3]), frozenset([2, 3, 4]), frozenset([1, 2, 3]), frozenset([4, 5])]}\ndf = pd.DataFrame(data)\n\n# Grouping by the column_of_sets\ngrouped = df.groupby('column_of_sets')\n\n# Iterate through the groups\nfor group, group_df in grouped:\n    print(\"Group:\", group)\n    print(group_df)\n    print(\"\\n\")","metadata":{"id":"PkGkluTZeHqN","outputId":"3251aed4-e7e7-467c-c8f4-b5b3b322f00f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_random_sampling(qa_dataframe, minPositive=None, noPositive=None, noNegative=None):\n  valid_api=[]\n  Corpus_dict = {}\n  Answers_dict = {}\n  Passage_dict = {}\n\n  gps_by_answer = qa_dataframe.groupby('set_answer')\n  invalid_api_cnt = 0\n\n  for k,v in gps_by_answer:\n    if len(v)>=minPositive:\n      valid_api.append(k)\n      print(k, ':', len(v)) # check number of each valid sample (a sample with the answer appears at least minPositive time)\n    else:\n      invalid_api_cnt += 1\n\n  # print(api_cnt, invalid_api_cnt)\n  print('number of valid api:', len(valid_api))\n\n  qa_dataframe_filtered = qa_dataframe[qa_dataframe['set_answer'].isin(valid_api)].reset_index(drop=True)\n  display(qa_dataframe_filtered)\n\n  gps_by_answer_filtered = qa_dataframe_filtered.groupby('set_answer')\n\n  title_list = qa_dataframe_filtered['title'].to_list()\n  answer_list = qa_dataframe_filtered['set_answer'].to_list()\n  print('len title_list:', len(title_list))\n  print('len answer_list:', len(answer_list))\n\n  for idx, tmp in enumerate(title_list):\n    Corpus_dict[idx] = tmp\n  for idx, tmp in enumerate(answer_list):\n    Answers_dict[idx] = tmp\n\n  for idx in range(len(title_list)):\n    label = Answers_dict[idx]\n\n    gp_of_label = gps_by_answer_filtered.get_group(label)\n\n    same_api_idx_gp = list(gp_of_label.index)\n\n    diff_api_idx_gp = list(set(range(len(answer_list)))-set(same_api_idx_gp))\n\n    if len(same_api_idx_gp)>noPositive:\n      same_api_idx_gp = sample(same_api_idx_gp, noPositive)\n\n    diff_api_idx_gp = sample(diff_api_idx_gp, noNegative)\n\n    Passage_dict[idx] = [same_api_idx_gp, diff_api_idx_gp]\n\n  print('len Corpus_dict', len(Corpus_dict))\n  print('len Answers_dict', len(Answers_dict))\n  print('len Passage_dict', len(Passage_dict))\n  display(Passage_dict)\n  return qa_dataframe_filtered, Corpus_dict, Answers_dict, Passage_dict\n","metadata":{"id":"3oVC4QlHhOzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_filtered, Corpus_dict, Answers_dict, Passage_dict = generate_random_sampling(df_train, 5, 10, 10)\n# test generate_random_sampling func","metadata":{"id":"-hFDtZ2gji96","outputId":"3f98e967-df84-460d-c1e5-6546a05cd912"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_triplets(Passage_dict):\n    Triplets= []\n    for k, v in Passage_dict.items():\n        for x in v[0]:\n            for y in v[1]:\n                Triplets.append([k,x,y])\n    return Triplets","metadata":{"id":"LwQPH6imJs9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rel_doc(df,evaluate_queries, Answers_dict):\n    rel_dict ={}\n    gps = df.groupby(\"set_answer\")\n\n    for q in evaluate_queries:\n        label = Answers_dict[q]\n\n        gp = gps.get_group(label)\n\n        same_api_idx_gp = list(gp.index)\n#         print(same_api_idx_gp)\n#         print(q)\n        same_api_idx_gp = list(set(same_api_idx_gp)-set([q]))\n\n        rel_dict[q] = [same_api_idx_gp]\n    return rel_dict","metadata":{"id":"FBAm22SqJxuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# haven't done\n# read the full data stored part of moshii\nevaluate_queries = sample(list(Passage_dict.keys()),1000)\nprint(\"evaluate_queries\",len(evaluate_queries))\nevaluate_Corpus = list(set(range(len(Answers_dict)))-set(evaluate_queries))\nevaluate_rel_doc = get_rel_doc(df_train_filtered,evaluate_queries,Answers_dict)\n\nwith open(f\"{evaluate_data_folder}/evaluate_queries.json\", 'w') as jsonfile:\n    json.dump(evaluate_queries, jsonfile)\n\nwith open(evaluate_data_folder+'evaluate_Corpus'+str(i)+'.json', 'w') as jsonfile:\n    json.dump(evaluate_Corpus, jsonfile)","metadata":{"id":"ipM7CxudeCpv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -------------------------------END---------------------------------------","metadata":{"id":"Oeff_5LZnM_Q"}},{"cell_type":"code","source":"import random\nfrom random import sample\nrandom.seed(1)\n\n\ndef generate_random_sampling_min_m_max_n(df,m=None,p=None,n=None):\n    Passage_dict = {}\n    Corpus_dict = {}\n    Answers_dict = {}\n    Triplets= []\n    api_list=[]\n\n    gps = df.groupby(\"set_answer\")\n\n    for k,v in gps:\n        if len(v)>=m:\n            api_list.append(k)\n    df = df[df[\"answer\"].isin(api_list)]\n    df = df.reset_index(drop=True)\n\n    print(\"len train\",len(df))\n\n    gps = df.groupby(\"answer\")\n\n    title_list = df[\"title\"].to_list()\n    answer_list = df[\"answer\"].to_list()\n\n    print(\"len title_list\",len(title_list))\n    print(\"len answer_list\",len(answer_list))\n\n    for idx,t in enumerate(title_list):\n        Corpus_dict[idx] = t\n\n    for idx,t in enumerate(answer_list):\n        Answers_dict[idx] = t\n\n    counter=0\n\n    for idx in range(len(title_list)):\n        counter+=1\n        if counter %10000 == 0: print(counter)\n        label = Answers_dict[idx]\n\n        gp = gps.get_group(label)\n\n        same_api_idx_gp = list(gp.index)\n        diff_api_idx_gp = list(set(range(len(answer_list)))-set(same_api_idx_gp))\n\n        if len(same_api_idx_gp)>p:\n            same_api_idx_gp=sample(same_api_idx_gp,p)\n\n        diff_api_idx_gp=sample(diff_api_idx_gp,n)\n\n        Passage_dict[idx] = [same_api_idx_gp,diff_api_idx_gp]\n\n    print(\"len Corpus_dict\",len(Corpus_dict))\n    print(\"len Answers_dict\",len(Answers_dict))\n    print(\"len Passage_dict\",len(Passage_dict))\n    return df,Corpus_dict,Answers_dict,Passage_dict\n\ndef get_triplets(Passage_dict):\n    Triplets= []\n    for k, v in Passage_dict.items():\n        for x in v[0]:\n            for y in v[1]:\n                Triplets.append([k,x,y])\n    return Triplets\n\ndef get_rel_doc(df,evaluate_queries, Answers_dict): # get all the relevant documents (posts with the same answer) for each query in the evaluation set\n    rel_dict ={}\n    gps = df.groupby(\"answer\")\n\n    for q in evaluate_queries:\n        label = Answers_dict[q]\n\n        gp = gps.get_group(label)\n\n        same_api_idx_gp = list(gp.index)\n#         print(same_api_idx_gp)\n#         print(q)\n        same_api_idx_gp = list(set(same_api_idx_gp)-set([q]))\n\n        rel_dict[q] = [same_api_idx_gp]\n    return rel_dict","metadata":{"id":"xjvIBzM6Kdqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p=10\nn=10\ndf_train_filtered,Corpus_dict,Answers_dict,Passage_dict = generate_random_sampling_min_m_max_n(df_train,m=5,p=p,n=n)","metadata":{"id":"iWKqLVGKKiBQ","outputId":"151d929b-5aeb-4840-8981-d3b0a71a4f95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ndf_train_answer_dict = {}\n\ndf_train_answer = list(set(df_train[\"answer\"].to_list()))\n\nfor idx, t in enumerate(df_train_answer):\n  df_train_answer_dict[idx] = t\n\nwith open(evaluate_data_folder+'/df_train_answer.json', 'w') as jsonfile:\n    json.dump(df_train_answer_dict, jsonfile)\nprint(\"df_train_answer.json has been created\")","metadata":{"id":"N5W4gQmjF9Fz","outputId":"199d59f4-2137-4253-f89b-547f6a02ae62"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Explore Answers_dict","metadata":{"id":"Teo7CI24wZSM"}},{"cell_type":"code","source":"Answers_dict_json = {}\ncollection_filepath = os.path.join(processed_data_folder, 'Answers_dict.json')\nwith open(collection_filepath, 'r', encoding='utf8') as fIn:\n  Answers_dict_json = json.load(fIn)","metadata":{"id":"GkIStL6Uw-bo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(Answers_dict_json)","metadata":{"id":"_scT90XEcRxU","outputId":"07c075e8-c3a7-4656-8bc7-75d7ccf476a4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Answers_dict_json","metadata":{"id":"F4x9lJWiyEjy","outputId":"3b7ca146-d1b7-4df0-d08f-9a51d67e6c4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# not use :D\n# convert strings to lists\nparsed_answers_dict = {k: eval(v) for k, v in Answers_dict.items()}\nparsed_answers_dict","metadata":{"id":"LvyYIcSEzikH","outputId":"f1dca486-b39d-47d7-d310-1c97350f7a64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count number of unique answer\nunique_answers_cnt = 0\nseen_answers = set()\nfor k, v in Answers_dict.items():\n  if v not in seen_answers:\n    seen_answers.add(v)\n    unique_answers_cnt += 1\nprint(unique_answers_cnt)","metadata":{"id":"1AmA4fdC0tLM","outputId":"973dc895-142b-487e-d9fd-789cea29ded2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Process data","metadata":{"id":"uu2E8ellv1II"}},{"cell_type":"markdown","source":"## 3.1 Requirements","metadata":{"id":"kv_eb1gzv_fL"}},{"cell_type":"code","source":"import random\nfrom random import sample\nrandom.seed(1)","metadata":{"id":"EiiRE8_Ov-ge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Generate random sampling","metadata":{"id":"Hno3bzuy5bKD"}},{"cell_type":"code","source":"def generate_random_sampling(qa_dataframe, minPositive=None, noPositive=None, noNegative=None):\n  valid_api=[]\n  Corpus_dict = {}\n  Answers_dict = {}\n  Passage_dict = {}\n\n  gps_by_answer = qa_dataframe.groupby('answer')\n  invalid_api_cnt = 0\n\n  for k,v in gps_by_answer:\n    if len(v)>=minPositive:\n      valid_api.append(k)\n      print(k, ':', len(v)) # check number of each valid sample (a sample with the answer appears at least minPositive time)\n    else:\n      invalid_api_cnt += 1\n\n  # print(api_cnt, invalid_api_cnt)\n  print('number of valid api:', len(valid_api))\n\n  qa_dataframe_filtered = qa_dataframe[qa_dataframe['answer'].isin(valid_api)].reset_index(drop=True)\n  display(qa_dataframe_filtered)\n\n  gps_by_answer_filtered = qa_dataframe_filtered.groupby('answer')\n\n  title_list = qa_dataframe_filtered['title'].to_list()\n  answer_list = qa_dataframe_filtered['answer'].to_list()\n  print('len title_list:', len(title_list))\n  print('len answer_list:', len(answer_list))\n\n  for idx, tmp in enumerate(title_list):\n    Corpus_dict[idx] = tmp\n  for idx, tmp in enumerate(answer_list):\n    Answers_dict[idx] = tmp\n\n  for idx in range(len(title_list)):\n    label = Answers_dict[idx]\n\n    gp_of_label = gps_by_answer_filtered.get_group(label)\n\n    same_api_idx_gp = list(gp_of_label)\n    diff_api_idx_gp = list(set(range(len(answer_list)))-set(same_api_idx_gp))\n\n    if len(same_api_idx_gp)>noPositive:\n      same_api_idx_gp = sample(same_api_idx_gp, noPositive)\n\n    diff_api_idx_gp = sample(diff_api_idx_gp, noNegative)\n\n    Passage_dict[idx] = [same_api_idx_gp, diff_api_idx_gp]\n\n  print('len Corpus_dict', len(Corpus_dict))\n  print('len Answers_dict', len(Answers_dict))\n  print('len Passage_dict', len(Passage_dict))\n  return qa_dataframe_filtered, Corpus_dict, Answers_dict, Passage_dict\n","metadata":{"id":"d3kyWOpQ5lr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test generate_random_sampling func\ndf_train1, Corpus_dict1, Answers_dict1, Passage_dict1 = generate_random_sampling(pair_dataset_filtered, 10, 10, 10)","metadata":{"id":"tPmerpklhODK","outputId":"9bfc1fd9-3247-425d-bccc-1d4ef16a3e30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test generate_random_sampling func\ndf_train, Corpus_dict, Answers_dict, Passage_dict = generate_random_sampling(pair_dataset_filtered, 5, 10, 10)","metadata":{"id":"bZlzH5i568O8","outputId":"088740e9-4596-4b6a-89f6-01c765583872"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"=> only 955 api is valid (which is having more than 4 queries about it)","metadata":{"id":"YYPt5m66uhbx"}},{"cell_type":"markdown","source":"=> Answers_dict.json also has 955 unique answers. Interesting!","metadata":{"id":"xt5lspuQ6DdH"}},{"cell_type":"code","source":"print(len(list(pair_dataset_filtered['answer'].unique())))","metadata":{"id":"HVsO7DOhuyZ-","outputId":"3de61ea7-b149-4756-88a8-f5d1d405e124"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Get triplets","metadata":{"id":"YXVYaL7UeSb7"}},{"cell_type":"code","source":"def get_triplets(Passage_dict):\n    Triplets= []\n    for k, v in Passage_dict.items():\n        for x in v[0]:\n            for y in v[1]:\n                Triplets.append([k,x,y])\n    return Triplets","metadata":{"id":"xRmKMJaAeNIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Get relevant documents for each query","metadata":{"id":"JNCy3URPfYSC"}},{"cell_type":"markdown","source":"Get all the relevant documents (posts with the samme answer) for  each query in the evaluation set","metadata":{"id":"YeTUAxbVgjLk"}},{"cell_type":"code","source":"def get_rel_doc(df,evaluate_queries, Answers_dict): # get all the relevant documents (posts with the same answer) for each query in the evaluation set\n    rel_dict ={}\n    gps = df.groupby(\"answer\")\n\n    for q in evaluate_queries:\n        label = Answers_dict[q]\n\n        gp = gps.get_group(label)\n\n        same_api_idx_gp = list(gp.index) #get all index of sample with same api\n#         print(same_api_idx_gp)\n#         print(q)\n        same_api_idx_gp = list(set(same_api_idx_gp)-set([q])) #remove itself index from the list\n\n        rel_dict[q] = [same_api_idx_gp]\n    return rel_dict","metadata":{"id":"pENih5NBgn2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nprint(set(range(5))-set())","metadata":{"id":"w5F7h51v3gwB","outputId":"f34a8e5e-5f77-4429-be46-263e1c1cefda"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Create evaluate set","metadata":{"id":"zQN4qgtmk1bl"}},{"cell_type":"code","source":"evaluate_queries = sample(list(Passage_dict.keys()),1000)\nprint(\"evaluate_queries\",len(evaluate_queries))\nevaluate_Corpus = list(set(range(len(Answers_dict)))-set(evaluate_queries))\nevaluate_rel_doc = get_rel_doc(df_train_filtered,evaluate_queries,Answers_dict)\n\nfolder = \"random_dis_query\"\nif not os.path.exists(folder):\n    os.mkdir(folder)\ndata_foler = folder+\"/\"\n\nfor i in range(10):\n    evaluate_queries = sample(list(Passage_dict.keys()),1000)\n    print(\"evaluate_queries\",len(evaluate_queries))\n    evaluate_Corpus = list(set(range(len(Answers_dict)))-set(evaluate_queries))\n    evaluate_rel_doc = get_rel_doc(df_train_filtered,evaluate_queries,Answers_dict)\n    with open(data_foler+'evaluate_queries'+str(i)+'.json', 'w') as jsonfile:\n        json.dump(evaluate_queries, jsonfile)\n\n    with open(data_foler+'evaluate_Corpus'+str(i)+'.json', 'w') as jsonfile:\n        json.dump(evaluate_Corpus, jsonfile)\n","metadata":{"id":"jW6Jf13oi2pB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# moshi code, paste here just to see and verify with my own code\ndef generate_random_sampling_min_m_max_n(df,m=None,p=None,n=None):\n    Passage_dict = {}\n    Corpus_dict = {}\n    Answers_dict = {}\n    Triplets= []\n    api_list=[]\n\n    gps = df.groupby(\"answer\")\n\n    for k,v in gps:\n        if len(v)>=m:\n            api_list.append(k)\n    df = df[df[\"answer\"].isin(api_list)]\n    df = df.reset_index(drop=True)\n\n    print(\"len train\",len(df_train))\n\n    gps = df.groupby(\"answer\")\n\n    title_list = df[\"title\"].to_list()\n    answer_list = df[\"answer\"].to_list()\n\n    print(\"len title_list\",len(title_list))\n    print(\"len answer_list\",len(answer_list))\n\n    for idx,t in enumerate(title_list):\n        Corpus_dict[idx] = t\n\n    for idx,t in enumerate(answer_list):\n        Answers_dict[idx] = t\n\n    counter=0\n\n    for idx in range(len(title_list)):\n        counter+=1\n        if counter %10000 == 0: print(counter)\n        label = Answers_dict[idx]\n\n        gp = gps.get_group(label)\n\n        same_api_idx_gp = list(gp.index)\n        diff_api_idx_gp = list(set(range(len(answer_list)))-set(same_api_idx_gp))\n\n        if len(same_api_idx_gp)>p:\n            same_api_idx_gp=sample(same_api_idx_gp,p)\n\n        diff_api_idx_gp=sample(diff_api_idx_gp,n)\n\n        Passage_dict[idx] = [same_api_idx_gp,diff_api_idx_gp]\n\n    print(\"len Corpus_dict\",len(Corpus_dict))\n    print(\"len Answers_dict\",len(Answers_dict))\n    print(\"len Passage_dict\",len(Passage_dict))\n    return df,Corpus_dict,Answers_dict,Passage_dict","metadata":{"id":"dyNmYxiXx6Qn"},"execution_count":null,"outputs":[]}]}